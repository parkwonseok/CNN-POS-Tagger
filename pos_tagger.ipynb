{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "tf.set_random_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초성 리스트. 00 ~ 18 --> 19개\n",
    "CHOSUNG_LIST = ['ᄀ', 'ᄁ', 'ᄂ', 'ᄃ', 'ᄄ', 'ᄅ', 'ᄆ', 'ᄇ', 'ᄈ', 'ᄉ', 'ᄊ', 'ᄋ', 'ᄌ', 'ᄍ', 'ᄎ', 'ᄏ', 'ᄐ', 'ᄑ', 'ᄒ']\n",
    "\n",
    "# 중성 리스트. 00 ~ 20 --> 21개\n",
    "JUNGSUNG_LIST = ['ᅡ', 'ᅢ', 'ᅣ', 'ᅤ', 'ᅥ', 'ᅦ', 'ᅧ', 'ᅨ', 'ᅩ', 'ᅪ', 'ᅫ', 'ᅬ', 'ᅭ', 'ᅮ', 'ᅯ', 'ᅰ', 'ᅱ', 'ᅲ', 'ᅳ', 'ᅴ',\n",
    "                 'ᅵ']\n",
    "\n",
    "# 종성 리스트. 00 ~ 27 + 1(1개는 종성없음코드) --> 28개\n",
    "JONGSUNG_LIST = [' ', 'ᆨ', 'ᆩ', 'ᆪ', 'ᆫ', 'ᆬ', 'ᆭ', 'ᆮ', 'ᆯ', 'ᆰ', 'ᆱ', 'ᆲ', 'ᆳ', 'ᆴ', 'ᆵ', 'ᆶ', 'ᆷ', 'ᆸ', 'ᆹ', 'ᆺ',\n",
    "                 'ᆻ', 'ᆼ', 'ᆽ', 'ᆾ', 'ᆿ', 'ᇀ', 'ᇁ', 'ᇂ']\n",
    "\n",
    "# 독립 자소 리스트. --> 51개\n",
    "INDI_LIST = ['ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅄ', 'ㅅ',\n",
    "             'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', 'ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ',\n",
    "             'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "\n",
    "hangul_johab = range(44032,55204)\n",
    "hangul_jaeum = range(12593,12623)\n",
    "hangul_moeum = range(12623,12644)\n",
    "hangul_chosung = range(4352,4371)\n",
    "hangul_jungsung = range(4449,4470)\n",
    "hangul_jongsung = range(4520,4547)\n",
    "english1 = range(65,91)\n",
    "english2 = range(97,123)\n",
    "digit = range(48,58)\n",
    "special_char = [ord('.'), ord('\\''), ord('?'), ord(','), ord('!'), ord('%')] # 형태소 분석에 필요하다고 생각하는 특수문자 추가\n",
    "\n",
    "def syllable(char):\n",
    "    s = ord(char) - 44032\n",
    "    cho = (s//21)//28\n",
    "    jung = (s%(21*28))//28\n",
    "    jong = (s%28)\n",
    "    \n",
    "    return CHOSUNG_LIST[cho], JUNGSUNG_LIST[jung], JONGSUNG_LIST[jong]\n",
    "\n",
    "def read_data(file_path):\n",
    "    sentence = []\n",
    "    data = []\n",
    "    label = []\n",
    "    d_append = data.append\n",
    "    with open(file_path,\"r\",encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            if line != '\\n':\n",
    "                w = line.split('\\t')\n",
    "                label.append(w[1].replace('\\n',''))\n",
    "                word = []\n",
    "                w_append = word.append\n",
    "                w_extend = word.extend\n",
    "                for c in w[0]:\n",
    "                    sign_unk = 0\n",
    "                    \n",
    "                    if ord(c) in hangul_johab or ord(c) in hangul_chosung or \\\n",
    "                       ord(c) in hangul_jungsung or ord(c) in hangul_jongsung or \\\n",
    "                       ord(c) in hangul_jaeum or ord(c) in hangul_moeum or \\\n",
    "                       ord(c) in english1 or ord(c) in english2 or \\\n",
    "                       ord(c) in digit or ord(c) in special_char: pass\n",
    "                    else: sign_unk = 1 # 지정된 한글, 영어, 숫자, 특수문자 이외에 전부 UNK태그 지정\n",
    "\n",
    "                    if sign_unk == 1:\n",
    "                        w_append('<UNK>')\n",
    "                    else:\n",
    "                        if ord(c) in hangul_johab: # 조합형 한글은 자모를 분리\n",
    "                            jaso = syllable(c)\n",
    "                            w_extend(jaso)\n",
    "                        else:\n",
    "                            w_append(c) # 한글자모, 영어, 숫자는 그대로\n",
    "                sentence.append((word,w[1].replace('\\n',''))) # ([분리된 형태소],태그) 형태로 저장\n",
    "            else:\n",
    "                if sentence != []:\n",
    "                    d_append(sentence) # sentence마다 구분지어서 저장\n",
    "                    sentence = []\n",
    "    return data,label\n",
    "\n",
    "def make_dictionary(label):\n",
    "    dictionary_char = dict()\n",
    "    dictionary_label= dict()\n",
    "    \n",
    "    char_list = ['<PAD>','<UNK>']+CHOSUNG_LIST+JUNGSUNG_LIST+JONGSUNG_LIST+INDI_LIST+[chr(i) for i in english1]\\\n",
    "               + [chr(i) for i in english2] + [chr(i) for i in digit] + [chr(i) for i in special_char]\n",
    "\n",
    "    for i in char_list:\n",
    "        dictionary_char[i] = len(dictionary_char)\n",
    "   \n",
    "    label = sorted(list(set(label)))\n",
    "    for i in label:\n",
    "        dictionary_label[i] = len(dictionary_label)\n",
    "    return dictionary_char, dictionary_label\n",
    "\n",
    "# 데이터를 index로 치환\n",
    "def make_dataSet(data, dictionary_char, dictionary_label):\n",
    "    indexed_data = [] \n",
    "    d_append = indexed_data.append\n",
    "    for sentence in data:\n",
    "        sen = []\n",
    "        s_append = sen.append\n",
    "        for word in sentence:\n",
    "            s_append(([dictionary_char[char] for char in word[0]], dictionary_label[word[1]]))\n",
    "        d_append(sen)\n",
    "    \n",
    "    return indexed_data\n",
    "\n",
    "data, label = read_data(\"data_v5_edit.txt\") # data는 3차원 리스트로 \n",
    "                                            # 전체 데이터 -> 문장 -> 형태소 순으로 저장됨\n",
    "dictionary_char, dictionary_label = make_dictionary(label)\n",
    "indexed_data = make_dataSet(data, dictionary_char, dictionary_label)\n",
    "\n",
    "pickle.dump(indexed_data,open('indexed_data.pkl','wb'))\n",
    "pickle.dump(dictionary_char,open('dictionary_char.pkl','wb'))\n",
    "pickle.dump(dictionary_label,open('dictionary_label.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_char = pickle.load(open('dictionary_char.pkl', 'rb'))\n",
    "dictionary_label = pickle.load(open('dictionary_label.pkl', 'rb'))\n",
    "indexed_data = pickle.load(open('indexed_data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_char_len = len(dictionary_char)\n",
    "dic_label_len = len(dictionary_label)\n",
    "\n",
    "word_max_len = 0\n",
    "for sentence in indexed_data:\n",
    "    for word in sentence:\n",
    "        if word_max_len < len(word[0]): \n",
    "            word_max_len = len(word[0])\n",
    "\n",
    "window_size = 7*2+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(?, 15, 48, 189, 1) dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# character composition model\n",
    "# CNN\n",
    "filter_num = 25\n",
    "X = tf.placeholder(tf.float32, [None, window_size, word_max_len, dic_char_len, 1])\n",
    "Y = tf.placeholder(tf.float32, [None, dic_label_len])\n",
    "dropout_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "# filter 3\n",
    "conv_layer1 = tf.layers.conv3d(X, filters=filter_num, kernel_size=[1, 3, dic_char_len], activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "conv_layer1 = tf.layers.max_pooling3d(conv_layer1, pool_size=[1, word_max_len-3+1, 1], strides=1)\n",
    "conv_layer1 = tf.layers.dropout(conv_layer1,rate = dropout_rate)\n",
    "# filter 5\n",
    "conv_layer2 = tf.layers.conv3d(X, filters=filter_num, kernel_size=[1, 5, dic_char_len], activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "conv_layer2 = tf.layers.max_pooling3d(conv_layer2, pool_size=[1, word_max_len-5+1, 1], strides=1)\n",
    "conv_layer2 = tf.layers.dropout(conv_layer2,rate = dropout_rate)\n",
    "# filter 7\n",
    "conv_layer3 = tf.layers.conv3d(X, filters=filter_num, kernel_size=[1, 7, dic_char_len], activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "conv_layer3 = tf.layers.max_pooling3d(conv_layer3, pool_size=[1, word_max_len-7+1, 1], strides=1)\n",
    "conv_layer3 = tf.layers.dropout(conv_layer3,rate = dropout_rate)\n",
    "# filter 9\n",
    "conv_layer4 = tf.layers.conv3d(X, filters=filter_num, kernel_size=[1, 9, dic_char_len], activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "conv_layer4 = tf.layers.max_pooling3d(conv_layer4, pool_size=[1, word_max_len-9+1, 1], strides=1)\n",
    "conv_layer4 = tf.layers.dropout(conv_layer4,rate = dropout_rate)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(?, 15, 100) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# character composition\n",
    "char_conv_concat = tf.concat([conv_layer4, conv_layer3, conv_layer2, conv_layer1], 4)\n",
    "char_conv_concat = tf.reshape(char_conv_concat,[-1, window_size, filter_num*4])\n",
    "char_conv_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context encoding model\n",
    "# binary features, position embedding\n",
    "batch=100\n",
    "bin_fea = [[[0],[0],[0],[0],[0],[0],[0],[1],[0],[0],[0],[0],[0],[0],[0]]]*batch\n",
    "binary_features = tf.constant(bin_fea, dtype=tf.float32)\n",
    "\n",
    "pos_emb = [[[1,0,0,1],\n",
    "            [1,0,1,0],\n",
    "            [1,0,1,1],\n",
    "            [1,1,0,0],\n",
    "            [1,1,0,1],\n",
    "            [1,1,1,0],\n",
    "            [1,1,1,1],\n",
    "            [0,0,0,0],\n",
    "            [0,0,0,1],\n",
    "            [0,0,1,0],\n",
    "            [0,0,1,1],\n",
    "            [0,1,0,0],\n",
    "            [0,1,0,1],\n",
    "            [0,1,1,0],\n",
    "            [0,1,1,1]]]*batch\n",
    "position_embedding = tf.constant(pos_emb, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims:0' shape=(100, 15, 105, 1) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context window\n",
    "window = tf.concat([char_conv_concat, position_embedding, binary_features],2)\n",
    "window = tf.expand_dims(window, -1)\n",
    "window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "filter_num = 128\n",
    "\n",
    "# filter 2\n",
    "conv_layer1 = tf.layers.conv2d(window, filters=filter_num, kernel_size=[2, 105], activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "conv_layer1 = tf.layers.dropout(conv_layer1,rate = dropout_rate)\n",
    "conv_layer1 = tf.layers.conv2d(conv_layer1, filters=filter_num, kernel_size=[2, 1], activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "conv_layer1 = tf.layers.max_pooling2d(conv_layer1, pool_size=[13, 1], strides=1)\n",
    "conv_layer1 = tf.layers.dropout(conv_layer1,rate = dropout_rate)\n",
    "# filter 3\n",
    "conv_layer2 = tf.layers.conv2d(window, filters=filter_num, kernel_size=[3, 105], activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "conv_layer2 = tf.layers.dropout(conv_layer2,rate = dropout_rate)\n",
    "conv_layer2 = tf.layers.conv2d(conv_layer2, filters=filter_num, kernel_size=[3, 1], activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "conv_layer2 = tf.layers.max_pooling2d(conv_layer2, pool_size=[11, 1], strides=1)\n",
    "conv_layer2 = tf.layers.dropout(conv_layer2,rate = dropout_rate)\n",
    "# filter 4\n",
    "conv_layer3 = tf.layers.conv2d(window, filters=filter_num, kernel_size=[4, 105], activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "conv_layer3 = tf.layers.dropout(conv_layer3,rate = dropout_rate)\n",
    "conv_layer3 = tf.layers.conv2d(conv_layer3, filters=filter_num, kernel_size=[4, 1], activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "conv_layer3 = tf.layers.max_pooling2d(conv_layer3, pool_size=[9, 1], strides=1)\n",
    "conv_layer3 = tf.layers.dropout(conv_layer3,rate = dropout_rate)\n",
    "# filter 5\n",
    "conv_layer4 = tf.layers.conv2d(window, filters=filter_num, kernel_size=[5, 105], activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "conv_layer4 = tf.layers.dropout(conv_layer4,rate = dropout_rate)\n",
    "conv_layer4 = tf.layers.conv2d(conv_layer4, filters=filter_num, kernel_size=[5, 1], activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "conv_layer4 = tf.layers.max_pooling2d(conv_layer4, pool_size=[7, 1], strides=1)\n",
    "conv_layer4 = tf.layers.dropout(conv_layer4,rate = dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_1:0' shape=(100, 512) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = tf.concat([conv_layer4, conv_layer3, conv_layer2, conv_layer1], 3)\n",
    "context = tf.reshape(context, [-1,filter_num*4])\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(100, 41) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fully Connected Layer\n",
    "W = tf.Variable(tf.truncated_normal([512, dic_label_len], stddev = 0.01))\n",
    "b = tf.Variable(tf.zeros([dic_label_len]))\n",
    "\n",
    "logit = tf.add(tf.matmul(context,W),b)\n",
    "Y_ = tf.nn.softmax(logit)\n",
    "Y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = Y, logits = logit))\n",
    "prediction = tf.equal(tf.argmax(Y_,1),tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction,tf.float32))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_data(indexed_data):\n",
    "    for sentence in indexed_data:\n",
    "        for w_index in range(len(sentence)):\n",
    "            X = np.zeros([15,48,189],dtype=np.float32)\n",
    "            Y = np.zeros([1,41], dtype=np.float32)\n",
    "            word_index = 5 - w_index\n",
    "            for word in sentence:\n",
    "                word_index = word_index + 1\n",
    "                if word_index >= 15 or word_index < 0: \n",
    "                    continue\n",
    "                if word_index == 6: \n",
    "                    Y[0][word[1]] = 1\n",
    "                if len(word[0]) % 2 == 0:\n",
    "                    char_index = 48//2 - len(word[0]) // 2\n",
    "                else:\n",
    "                    char_index = 48//2 - (len(word[0]) // 2 + 1)\n",
    "                \n",
    "                for i, char in enumerate(word[0]):\n",
    "                    X[word_index][int(i+char_index)][char] = 1\n",
    "            X = np.reshape(X, [1, 15, 48, 189, 1])\n",
    "            \n",
    "            yield X, Y\n",
    "            \n",
    "def indexed_data_size(indexed_data):\n",
    "    size = 0\n",
    "    for sentence in indexed_data:\n",
    "        size += len(sentence)\n",
    "    return size\n",
    "\n",
    "def make_batch_input(indexed_data, batch):\n",
    "    indexed_data = indexed_data[:3000]\n",
    "    total_size = indexed_data_size(indexed_data)\n",
    "    sentence_input = make_input_data(indexed_data)\n",
    "    for i in range(total_size//batch):\n",
    "        X_input = []\n",
    "        Y_input = []\n",
    "        for j in range(batch):\n",
    "            X, Y =next(sentence_input)\n",
    "            X_input.append(X)\n",
    "            Y_input.append(Y)\n",
    "        X_input = np.concatenate(X_input, 0)\n",
    "        Y_input = np.concatenate(Y_input, 0)\n",
    "        yield X_input, Y_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  :  accuracy : 0.2968   loss : 2.8725\n",
      "200  :  accuracy : 0.4394   loss : 2.0030\n",
      "300  :  accuracy : 0.6662   loss : 1.1722\n",
      "400  :  accuracy : 0.8066   loss : 0.6808\n",
      "500  :  accuracy : 0.8475   loss : 0.5308\n",
      "600  :  accuracy : 0.8886   loss : 0.3895\n",
      "700  :  accuracy : 0.9086   loss : 0.3154\n",
      "800  :  accuracy : 0.9210   loss : 0.2720\n",
      "900  :  accuracy : 0.9213   loss : 0.2559\n",
      "1000  :  accuracy : 0.9260   loss : 0.2414\n",
      "1100  :  accuracy : 0.9385   loss : 0.2127\n",
      "1200  :  accuracy : 0.9478   loss : 0.1784\n",
      "1300  :  accuracy : 0.9193   loss : 0.2448\n",
      "1400  :  accuracy : 0.9207   loss : 0.2869\n",
      "1500  :  accuracy : 0.8725   loss : 0.4128\n",
      "1600  :  accuracy : 0.9170   loss : 0.2749\n",
      "1700  :  accuracy : 0.9336   loss : 0.2169\n",
      "1800  :  accuracy : 0.9384   loss : 0.1979\n",
      "1900  :  accuracy : 0.9457   loss : 0.1814\n",
      "2000  :  accuracy : 0.9438   loss : 0.1905\n",
      "2100  :  accuracy : 0.9365   loss : 0.2265\n",
      "2200  :  accuracy : 0.9270   loss : 0.2466\n",
      "2300  :  accuracy : 0.9390   loss : 0.2020\n",
      "2400  :  accuracy : 0.9434   loss : 0.1842\n",
      "2500  :  accuracy : 0.9443   loss : 0.1747\n",
      "2600  :  accuracy : 0.9466   loss : 0.1731\n",
      "2700  :  accuracy : 0.9536   loss : 0.1551\n",
      "2800  :  accuracy : 0.9451   loss : 0.1767\n",
      "2900  :  accuracy : 0.9432   loss : 0.1778\n",
      "3000  :  accuracy : 0.9587   loss : 0.1308\n"
     ]
    }
   ],
   "source": [
    "# trainging\n",
    "batch = 100\n",
    "train_size = indexed_data_size(indexed_data[:3000])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    step=100\n",
    "    input_batch = make_batch_input(indexed_data, batch)\n",
    "    a_batch = []\n",
    "    c_batch = []\n",
    "\n",
    "    for i in range(train_size//batch):\n",
    "        X_input, Y_input = next(input_batch)\n",
    "        a, c, _ = sess.run([accuracy, cross_entropy, optimizer],feed_dict={X:X_input, Y:Y_input, dropout_rate:0.5})\n",
    "        a_batch.append(a)\n",
    "        c_batch.append(c)\n",
    "        if i % step == 0 and i != 0:\n",
    "            print(i, ' : ', 'accuracy : {:0.4f}   loss : {:0.4f}'.format(sum(a_batch)/step, sum(c_batch)/step))\n",
    "            a_batch = []\n",
    "            c_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pws2]",
   "language": "python",
   "name": "conda-env-pws2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
